\chapter {Systems History}

\section{Timeline of Papers}

\scalebox{1}{
\begin{tabular}{r |@{\foo} l}

1952 & The Education of a Computer \cite{hopper1952education} \\
1957 & The FORTRAN Automatic Coding System \cite{backus1957fortran} \\
1959 & Recursive Functions of Symbolic Expressions and Their Computation by Machine \cite{mccarthy1959recursive} \\
1964 & Architecture of the IBM System/ 360 \cite{amdahl1964architecture} \\
1965 & Cramming More Components onto Integrated Circuits \cite{moore1965cramming} \\
1967 & Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities \cite{amdahl1967validity} \\
1978 & The UNIX Time- Sharing System \cite{ritchie1978unix} \\
1979 & Experience with Process and Monitors in Mesa \cite{lampson1980experience} \\
1980 & The Case for the Reduced Instruction Set Computer \cite{patterson1980case} \\
     & Comments on the Case for RISC \cite{clark1980comments} \\
1982 & Gprof: A call graph execution profiler \cite{graham1982gprof} \\
1983 & Hints for Computer System Design \cite{lampson1983hints} \\
1988 & Garbage Collection in an Uncooperative Environment \cite{boehm1988garbage} \\
1989 & Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines \cite{jouppi1989available} \\
2008 & Amdahlâ€™s Law in the Multicore Era \cite{hill2008amdahl} \\
2017 & Bringing the Web Up to Speed with WebAssembly \cite{haas2017bringing} \\

\end{tabular}
}

\newpage

\section {Template to Copy}

\paragraph{\textbf{Summary}}
\paragraph{\textbf{Context}}
\paragraph{\textbf{Discussion Points}}
\paragraph{\textbf{Significance}}

\section {The Education of a Computer \cite{hopper1952education}}

\paragraph{\textbf{Summary}}
This paper goes through iterations of "educating" a computer, to where the
mathematician using a computer gets the boot, the computer becomes the
mathematician, and the programmer becomes an integral part of the computer. A
subroutine is a function-like that performs some computation. It has an entry
line, exit line, result line, argument lines, and routine lines. Different
procedures interact with different lines of the subroutines. The computer is
given a bunch of smaller mathematical subroutines that the programmer can use to
help construct their programs. It is hypothesised that more "subroutines" could
be developed and combined. It is unclear to me what the correlation is between
subroutines and modern day functions.
\paragraph{\textbf{Context}}
I think this work is difficult for modern day programmers to understand/ fathom
(myself included). It is difficult to think of what may lie between
assembly-style jumps and Haskell, for example, which is the space that this
paper explores.
\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item Page 245 discusses turning using programs that contain subroutines as
    a subroutines itself, which I would consider a profound observation. Yet the
    "conclusion" focuses just on the arithmetic type mathematical advanecs. Hmm.
    \item The proposed UNIVAC is claimed to "not forget" and "not make
    mistakes." We all know that that is not the case.
\end{enumerate}


\section {The FORTRAN Automatic Coding System \cite{backus1957fortran}}

\paragraph{\textbf{Summary}}
The paper proposes FORTRAN, a language resulting from a need to reduce computing
overhead. The authors report that the goal of FORTRAN is for programmers to be
able to use a concise language to specify procedures, rather than writing the
granual 704 code directly. FORTRAN achieves this by introducing both a high
level language a mechanism of automatic translation to produce optimized 704
code. The language presented uses statements that modern day programmers would
recognize - functions, loops, print, etc.

The technical contributions are quite great. The FORTRAN language itself
introduces new statements that programmers can use to write pseudo 704 code,
with each FORTRAN statement translating to 4 to 20 704 statements. This greatly
reduces the time and effort required to write programs and debug. The FORTRAN
translator automatically translates FORTRAN statements to 704 statements, and
while doing so it makes a number of optimizations that allow for efficient 704
code to be produces. This 704 code may even be more efficient than what a
programmer may have wrote directly, and the translator is able to detect
patterns and optimizations for different high level ideas. The translator also
is able to catch a number of errors and help the programmer debug.
\paragraph{\textbf{Context}}
This paper's goal was to develop a tool to make programming easier, and it does
so by developing a high level representation of frequently used code and
introducing a method of translating that high level representation to optimized
low level code automatically. Really, that goal hasn't changed much since this
time. Obviously languages and toolchains have evolved, but most modern languages
employ this automatic translation/ optimization process, and for good reason. It
is likely that this paper laid a large portion of the foundation for modern
languages.
\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item Not a discussion question, but I belive last class it was said that
    FORTRAN was originally intended for researchers. In retrospect it is easy to
    see why FORTRAN-like languages would be appealing for most programmers, not
    just researchers, but I am interested in how this transition arose at the
    time.
    \item Hmm, I actually keep getting kind of confused by the use of
    "subroutine" in each of these papers, how is a subroutine different than a
    function? I read the other paper for this week first, and I was mentally
    translating "subroutine" to "function", but this paper makes a distinction.
    Can we clarify the difference?
\end{enumerate}
\paragraph{\textbf{Significance}}
This paper was definitely significant, as it likely paved the way for most
modern languages. Not only does it introduce two major components, the lanaguage
representation and the compiler, but it also presents a very compelling case for
why each of these is useful. This paper likely introduced a lot of motivation
for creating tools that could help programmers be more efficient and more
accurate.

\section {Recursive Functions of Symbolic Expressions and Their Computation by
Machine \cite{mccarthy1959recursive}}

\paragraph{\textbf{Summary}}
This paper defines a formalism for defining potentialy recursive functions and
proposes S-expressions, S-functions, and S-function apply, which together are a
system of theoretical program representation that is independent from its
machine implementation. The paper then discusses the representation of
S-expressions as lists in IBM 704 memory. This work addressed a need to
represent LISP symbolically, and it offered additional tools to be used in
mathematical logic and theorem proving.

The key ideas include: S-expressions, symbolic expressions composed of ".", "(",
")", and an infinite set of atomic symbols. Operations upon S-expressions.
M-expressions, or functions of S-expressions. S-functions, which are described
by M-expressions, and the S-function apply. The implementation of S-expressions
in 704 memory as lists, where the list structure is specifically mechanized to
implement an early iteration of garbage collection.
\paragraph{\textbf{Context}}
This paper discusses LISP for the IBM 704. Technologies have changed
dramatically, and we no longer use the IBM 704. LISP is still used, and I'm sure
its use cases have expanded past those listed in Section 4. S-expressions are
still frequently discussed, although I think because of their simplicity they
are most discussed in a pedagogical context. I ran into them when I was googling
about Rust macros.
\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item The description of the formalism and the methods reminded me of "An
    Introduction to the Theory of Lists" by Richard S. Bird, in the sense of,
    they both describe similar operations, one upon S-expressions and the other
    upon lists. Any relationship there, or no? This potential correlation seems
    most relavent to the section that discusses representing S-expressions as
    lists.
    \item It seems like the authors of this paper and the FORTRAN paper both
    naturally extended functions to include higher order functions. Maybe this
    is just me not understanding things, but I would have expected higher order
    functions to be some crazy difficult problem, but I guess that is not the
    case.
    \item Section 4 lists that LISP was used to write the compiler to compile
    LISP programs. I believe this is called "bootstrapping"? Is this the first
    instance of this? / I forget what the FORTRAN compiler is written in.
\end{enumerate}
\paragraph{\textbf{Significance}}
This paper primarily proposes a theoretical program representation and
secondarily presents the machine implementation. This is in contrast to the
FORTRAN paper, which integrated the "theoretical" language design into the
physical implementation, and likely had significant impact on mathematical
foundations of computer science. This paper also likely laid foundations for
garbage collection.


\section {Garbage Collection in an Uncooperative Environment
\cite{boehm1988garbage}}


\paragraph{\textbf{Summary}}
This paper discusses a method of garbage collection that does not require
any cooperation from the object code generated from the source program.
This means that garbage collection can be isolated - allowing for simplier
implementations in conventional compilers and for garbage collection
overhead to also be isolated such that programs that do not use garbage
collection no longer suffer from this incurred overhead.

The garbage collection algorithm presented is the mark-sweep algorithm,
which employs two passes over the data to perform collection while
gaurenteeing that accessible data is never corrupted. The first pass "marks"
all of the data that is accessible by the program and the second pass
returns inaccessible objects to a list of free memory.

The authors discuss several issues that arise when thinkng about garbage
collection, but the most notable of their observations is that the
problem of whether or not a particular item is accessed in any possible
iteration of the program is not decidable. 
\paragraph{\textbf{Context}}
Well, garbage collection is still used today, and it will continue to be
used for the forseeable future. I don't know enough about modern garbage
collection to synthesize how this paper may have influenced modern designs,
but the problems discussed in the paper are still problems faced today.
\paragraph{\textbf{Discussion Points}}
Last class we discussed NVM (spelling?), which was something along the
lines of "persistent memory". How does garbage collection work in this
environment?


\section {Bringing the Web Up to Speed with WebAssembly \cite{haas2017bringing}}


\paragraph{\textbf{Summary}}
This paper introduces WebAssembly, a portable low-level "language"
(bytecode) intended to be used across most modern web browsers. WebAssembly
is designed to be language, hardware, and platform independent, and it is
abstract over programming models. The authors introduce WebAssembly
semantics, memory access, control flow, and type system. They discuss
browser implementations and performance results. WebAssembly is slower than
native code but faster than asm.js, but it also is smaller in resulting
size.

The control flow and type system are two of the main technical contributions
from this paper. WebAssembly uses a structured control flow, where the
design bakes in the gaurentee that the code does not contain irreducible
loops or "bad" branches. Control flow elements monitor their own local
operand stacks, and branching in the code clears an operand stack, such that
users don't have to track information about the stack. This design allows
the type system to validate the code in a single pass.

The WebAssembly type system is interesting. It is written in such a way
that all of the reduction rules are sound. This means that type-correct
WebAssembly code contains so invalid calls, illegeal accesses, and is memory
safe. The type system is inherently connected to the control flow design -
the control flow design allows the type system to validate the code in one
pass and the type system implies that the layout of the operand stack is
determined statically. This establishes memory and state encapsulation.
\paragraph{\textbf{Context}}
WebAssembly is recent, this paper is just from 2017. I'm glad we read
this, I see lots of people online really excited about the implications of
WebAssembly. The control flow design is novel, although the idea of baking
into the type system some desired behavior is not novel.
\paragraph{\textbf{Discussion Points}}
1~) Why is JavaScript the only natively supported language on the Web "by
historical accident"?
2~) What would be the closest relatives to WebAssembly's type system?
3~) It seems like the most recent languages to be put into the spotlight
(Rust, WebAssembly) have core features baked into the type systems so that
their target behaviors can be statically determined. Should we expect this
to be an upcoming trend or is this coincidence?
\paragraph{\textbf{Significance}}
WebAssembly is very significant, and much needed. It opens up many
possibilities for what one might be able to do on the web, and it ensures
that those new possibilities are done relatively more safely. I am excited
to see more developments in WebAssembly.


\section {Architecture of the IBM System/ 360 \cite{amdahl1964architecture}}


\paragraph{\textbf{Summary}}
Amdahl et al. describes the architectural design of the IBM System/ 360 and
the rational behind each design choice. At a high level, the authors discuss
the goal of the system design - to provide a general purpose machine that
would provide an abstract data representation, agnostic from any particular
function or application.
\paragraph{\textbf{Context}}
This paper was published 4 years after the ALGOL 60 paper, another paper
working towards the goal of general purpose computing. In addition to being
revolutionary at the time of publication (I imagine), this paper also
introduced many of the hardware concepts that we know today - I/O, 32 bit
and 64 bit processors, etc.
\paragraph{\textbf{Discussion Points}}
1~) This paper details that one of the requirements of the new IBM System/
360 is that it would have to be such that "each individual model and
systems configuration in the line would have to be competitive with systems
that are specialzed in function, performance level or both." This is
interesting because it is difficult to imagine what the school-of-thought
must have been at the time, to where the inherent value of a more generic
machine is not recognized. I read this and was suprised that this was a
constraint, for this reason.
2~) Is it by accident that the IBM System/ 360 "got right" the concept of
32~-bit and 64~-bit systems? Or is it mearly an informed design choice, that
we have continued to use? Could there have been alternative designs that
work as well or better? What would the implications be for software
developement, for computing schools of thought?
\paragraph{\textbf{Significance}}
This paper is very significant, it introcued I/O and processor design and
likely changed the computing school of thought at the time.


\section {Cramming More Components onto Integrated Circuits
\cite{moore1965cramming}}


\paragraph{\textbf{Summary}}
This paper introduces Moore's Law, a theory stating the number of components
per integrated curcuit should double every year. This is a very well known
theory, so it is interesting to read the origin.

The paper also offers detailed motivation for why designers, engineers, or
consumers might want Moore's Law to be in effect - it makes everyone's lives
easier. Moore dispells some false ideas about electricty needs, and offers
possible design concepts.
\paragraph{\textbf{Context}}
This paper has remained quite relavent for some time. It outlived the
predictions that Moore himself makes in the paper.
\paragraph{\textbf{Discussion Points}}
1~) I hear a lot of talk about how Moore's Law is outdated and is becoming
irrelevant to modern design concerns. But a simple Google search suggests
that Moore's Law is still observable. What's up with that?
2~) The last section discusses "Linear circuitry". What is that?
3~) It is difficult to seperate correlation versus causation. Could it have
been the case that Moore's Law served as a subconcious guide or influence to
hardware engineers? As in, perhaps attention would have been focused
elsewhere, had engineers not already had the expectation that a 2x
improvement was possible.
\paragraph{\textbf{Significance}}


\section {Validity of the Single Processor Approach to Achieving Large Scale
Computing Capabilities \cite{amdahl1967validity}}


\paragraph{\textbf{Summary}}
This paper revisits the original publication that introduced Amdahl's Law,
a method of calculating prospective execution speed ups given certain
hardware improvements. The paper walks through the rational behind reaching
this conclusion.

This paper was short, and quite honestly I had a hard time following it, so
I'm struggling with a review. I did enjoy how sassy the editors note is
about this very thing: "Interestingly, it has no equations and only a single
figure."
\paragraph{\textbf{Discussion Points}}
1) What is Amdahl's Law in layman's terms?
\paragraph{\textbf{Significance}}
I can infer that Amdahl's Law is likely significant, based on the fact that
there was a reprint issued. I do not know what the modern perspective is on
it though.


\section {Amdahlâ€™s Law in the Multicore Era \cite{hill2008amdahl}}

\paragraph{\textbf{Summary}}
This paper applied Amdahl's Law to multicore chips, and to do so the authors
present a novel cost model that considers the number and performance of
cores. They analyze their cost model and demonstrate that it can be used to
apply Amdahl's Law to multicore chips. Using this new model, the authors
conclude that denser chips increase the likelihood that cores will lose
performance. They connect this to Moore's Law and urge researchers to find
ways to improve chip performance with denser chips.

They also analyze Amdahl's Law against asymmetric multicore chips, and find
that asymmetric chips have potential for greater speedups than symmetric
chips. They propose that Amdahl's Law may not be particularly adept to
modeling these asymmetric chips.
\paragraph{\textbf{Discussion Points}}
1) How does this relate to the "stacked" chip design that was mentioned in
class earlier this semester?


\section {The Case for the Reduced Instruction Set Computer
\cite{patterson1980case}}

\paragraph{\textbf{Summary}}
This paper offers motivation for why exploring reduced instruction set computers
(RISC) might be worthwhile or profitable. Patterson and Ditzel claim that the
trend towards higher level languages is supported by several nuanced factors.
They propose that it has been easier to gain a 10\% computation advance by
adding 10\% more hardware, rather than making code 10\% more dense, and this
plays into the fact that it is actually more adventageous for companies to
market `more advanced instruction sets', basically meaning that there is no
pressure on designers to actively reduce instruction sets.

The authors provided several pieces of evidence to offer insight into how more
complicated instruction sets might actually be detrimental to compiler writers
and assembly-language writers, and they propose why RISC might elleviate some of
these burdens.
\paragraph{\textbf{Context}}
I don't really know the context. I don't know what the modern stance is on RISC
versus CISC.
\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item Something that strikes me when reading these older papers is that it
    seems that the general mentality at the time was that there may not be a
    need or purpose for more complex or general-purpose computers, or maybe
    even, it seems that there was air of mysticism and uncertainty around how
    general-purpose computer technologies would unfold or would benefit users.
    This is demonstrated in this paper in the abstract: ``If we review the
    history of computer families we find that the most common architectural
    change is the trend toward ever more complex machines." Do I have a correct
    assessment of the situation?
    \item Given (1) is a correct assessment, it seems that modern technologies
    might be at a similar empass.
    \item Patternson and Ditzel's complaints about how code compaction interacts
    with marketing strategies seems to be similar to my own thoughts on machine
    learning and the use of giant machines for training by ML giants (DeepMind,
    Google AI, etc).
\end{enumerate}
\paragraph{\textbf{Significance}}

\section {Comments on the Case for RISC \cite{clark1980comments}}

\paragraph{\textbf{Summary}}
This paper critiqued The Case for the Reduced Instruction Set Computer
\cite{patterson1980case}, going through point by point describing by each
assertion made as incorrect or misguided. The authors make individual points,
but their overall critique seems to be that they feel the original paper is
baseless. Clark and Strecker argue that Patternson and Ditzel did not provide
sufficient evidence to back up their claims, and in fact offer evidence to the
contrary for many of them.

Clark and Strecker seem to believe that the case for RISC versus CISC is on a
very case-by-case basis. They state that it could be the case that more
specialzed, complex instructions have an advantage over combinations of smaller
instructions, in specific scenarios. They believe that instruction set
complexity cannot be measured by instruction count, contrary to the foundation
of Patternson and Ditzel's paper.

My favorite line was ``Annecdotal accounts of irrational implementations are
certainly interesting.''
\paragraph{\textbf{Context}}
This came out the same year as the original paper that it is critiquing.
\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item So what exactly is RISC versus CISC?
    \item What would all four of these authors think of modern lanaguages?
    \item Of these two paper, which is the more generally "accepted" one, and why?
\end{enumerate}
\paragraph{\textbf{Significance}}
It's unclear to me what the significance was of the original paper, but I
imagine the situation must be significant enough to warrant a published
critique.

\section {Available Instruction-Level Parallelism for Superscalar and
Superpipelined Machines \cite{jouppi1989available}}

\paragraph{\textbf{Summary}}
This paper analyzes the differences between machines with different parallelism
mechanisms. It proposes 5 different types of machines: baseline, underpipelined,
superscalar, superpipelined, VLIW, and superscalar + superpipelined. The two in
focus are superscalar and superpipelined, where superscalar machines executes n
instructions in a cycle and superpipelined machines execute 1 instruction every
1/n cycle. It is shown that although the two machines execute the same number of
instructions in the same number of cycles, the superpipelined machine has a
larger startup latency so the superscalar machine is actually 10\% better.

However, these are theoretical machines, and actual instructions are much more
complicated and contain multi-cycle instructions.

\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item If a language were to have something like, memory layout/ management/
    usage baked into the type system, wouldn't the language be able to maximize
    automatic parallelisation? Does this exist?
    \item I did not understand their explanation of VLIW machines, can we go
    over this?
\end{enumerate}

\section {Experience with Process and Monitors in Mesa
\cite{lampson1980experience}}

\paragraph{\textbf{Summary}}
Lampson and Redell discuss the integration of processes and monitors into Mesa,
and already existing programming language. The work is motivated by a desire to
implement parrallelization into Pilot, a program closely linked to Mesa. They
authors discuss the mechanism of implementation, the actual implementation in
Mesa, and applications that use Mesa.

The notable contributions are the developments made to integrate processes and
monitors with Mesa. Processes are integrated by means of `detach' and `join'.
A processes detaches from its caller and is then joined in later, at a specific
point in the code, once it has completed. This allows multiple processes to be
executed asynchronously. Monitors are used to protect data, and data gaurded by
a monitor can only be accessed within the body of the monitor procedure. A
`notify' system is used, where processes establish conditions upon which some
other process *may* be waiting for that condition to hold, and they sends out
notifications about these conditions. It is up to the notified process if they
perfectly meet that estimated condition or not. This allows one process waiting
on a condition to execute, but there also exists a broadcast operation that
allows all processes waiting on a condition to execute.

The authors develop this work explicitly without considering security. The
"system only supports one user", so the Mesa type system is sufficient. There is
still also the possibility of race conditions and deadlocks.
\paragraph{\textbf{Context}}
This paper came out right before the RISC/ CISC papers, but about 10 years
before the "Available Instruction-Level Parallelism for Superscalar and
Superpipelined Machines" paper.
\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item Did this paper contribute to the modern idea of Futures? The
    description of Processes sounded eerily similar to Futures, although I don't
    believe Futures employ the notify system.    
\end{enumerate}

\section {The UNIX Time- Sharing System \cite{ritchie1978unix}}

\paragraph{\textbf{Summary}}
Ritchie and Thompson present the design of the UNIX system. They cover in detail
the different aspects of the computer's design (file system, I/O, asynchronous
processes, the shell) and discuss their importance and contribution to the UNIX
system, and discuss

As pointed out by the authors, it is not necessarily that the UNIX system
implemented any particularly insane features, its that it does so in a way that
is sustainable and abstract. Perhaps most importantly, the users are able to
modify the system itself, as all system files are available. This allows users
resolve errors quickly. The OS is modular and abstract and handles many
processes for the user automatically - address spaces are organized by program,
the OS implements I/O so that the user can use one I/O end-point, and others.

\paragraph{\textbf{Context}}
From the papers we have read, it seems that this paper came out after a bit of a
lull. It was published 11 years after Validity of the Single Processor Approach
to Achieving Large Scale Computing Capabilities, but then the MESA and RISC/
CICS papers shortly after.

\paragraph{\textbf{Discussion Points}}
The paper discusses how severe size constraits on the system lead to
innovations. Given what we discussed last week - that how modern hardware
is sold by ``features'' - it seems to me that currently there are not enough
imposed pressures upon hardware to foster innovations. This is perhaps akin
to natural selection.
\begin{enumerate}
    \item How might different imposed pressures have lead to different modern
    hardware?
    \item Is our hardware development timeline `inevitable'?
\end{enumerate}

\paragraph{\textbf{Significance}}
UNIX is widely used today in different forms. Luckily it does not cost \$40,000
per installation.

\section {Hints for Computer System Design \cite{lampson1983hints}}

\paragraph{\textbf{Summary}}
This paper was pretty straightforward, not a lot of technical detail. In my own
words, I would summarize the message into a few key points.

Make components of the system as simple as they need to be, but not more. 
Components should be modular and ``pure''. As in, each component should be a
dedicated layer of abstraction that is focused on performing one particular
task. Artifacting should be reduced across components. Error and faults should
be handled modularly.

A good and complete system is better than a great and incomplete system. All
components of the system should be fleshed out, reliable, and safe. Components
should not be to abstract or general to where they cannot handle particular
cases.

Users care about a working system more than they care about the most
state-of-the-art technically-advanced system.

\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item Are any of these guidelines particularly important over the others?
\end{enumerate}

\paragraph{\textbf{Significance}}
These are good guidelines. I suspect I will revist this paper in the future.

\section {Gprof: A call graph execution profiler \cite{graham1982gprof}}

\paragraph{\textbf{Summary}}
This paper presents gprof, a program evaluation tool that focuses on presenting
information about execution organiztion and execution time. This is done by
analyzing program call graphs, where the nodes of the call graphs are
sub-routines and the arcs between nodes are the "who called whom." Gprof
collects both execution counts and execution times to complete the suite,
although both of these present their own unique implementation challenges. The
resulting evaluation is presented to the user to help them analyze either
components of their own programs or components of unknown programs.

Gprof is designed to be as lightweight as possible. Only minimal logging
information is captured during dynamic execution. Once a program completes,
gprof completes the data collection and analysis and only then performs the
computationally intensive components. The authors discuss how to transform
large recursion call graphs into a set of strong connected components, but they
note that this fails for *very* large recursion call graphs.

\paragraph{\textbf{Context}}
The next paper for this week references gprof so it must be important. Also,
recently I came across a paper that implemented a tracing JIT for Racket that
used call graphs and continuation frames to detect recursion while tracing. I
suspect that their work is derived in some capacity from this work.

\paragraph{\textbf{Discussion Points}}
\begin{enumerate}
    \item This evalution tool seems useful. Perhaps this is what the second
    paper covers, but this tool is not compatible with current technologies and
    architectural toolchains, so to speak. One of the systems lunch speakers
    discussed how decreasing the variance in latencies for a component of a
    serverless system could decrease the average end-to-end latency. That being
    said, I don't believe the reported metrics from gprof are detailed enough to
    express these complicated relationships.
\end{enumerate}

\paragraph{\textbf{Significance}}
I do not know the significance. I suspect that this is an important paper.
Although this paper presents an evaluation system, its methodology is
application-neutral.